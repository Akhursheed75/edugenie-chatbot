2025-06-28 22:51:39,012 - DEBUG - Using proactor: IocpProactor
2025-06-28 22:51:39,012 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-28 22:51:39,043 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-28 22:51:39,249 - DEBUG - Launching Gradio app...
2025-06-28 22:51:39,380 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000270BE53DD20>
2025-06-28 22:51:39,380 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000270BE38FEC0> server_hostname='api.gradio.app' timeout=3
2025-06-28 22:51:39,396 - DEBUG - Using proactor: IocpProactor
2025-06-28 22:51:39,449 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None
2025-06-28 22:51:39,449 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000270BE941B70>
2025-06-28 22:51:39,449 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-28 22:51:39,449 - DEBUG - send_request_headers.complete
2025-06-28 22:51:39,449 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-28 22:51:39,449 - DEBUG - send_request_body.complete
2025-06-28 22:51:39,449 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-28 22:51:39,449 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 17:51:39 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-28 22:51:39,449 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-28 22:51:39,449 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-28 22:51:39,449 - DEBUG - receive_response_body.complete
2025-06-28 22:51:39,464 - DEBUG - response_closed.started
2025-06-28 22:51:39,464 - DEBUG - response_closed.complete
2025-06-28 22:51:39,464 - DEBUG - close.started
2025-06-28 22:51:39,464 - DEBUG - close.complete
2025-06-28 22:51:39,467 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None
2025-06-28 22:51:39,467 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000270BE942F50>
2025-06-28 22:51:39,468 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-28 22:51:39,469 - DEBUG - send_request_headers.complete
2025-06-28 22:51:39,469 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-28 22:51:39,469 - DEBUG - send_request_body.complete
2025-06-28 22:51:39,471 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-28 22:51:39,471 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-28 22:51:39,499 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 17:51:39 GMT'), (b'server', b'uvicorn'), (b'content-length', b'24684'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-28 22:51:39,500 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-06-28 22:51:39,500 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-28 22:51:39,500 - DEBUG - receive_response_body.complete
2025-06-28 22:51:39,500 - DEBUG - response_closed.started
2025-06-28 22:51:39,500 - DEBUG - response_closed.complete
2025-06-28 22:51:39,500 - DEBUG - close.started
2025-06-28 22:51:39,501 - DEBUG - close.complete
2025-06-28 22:51:39,505 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-28 22:51:39,812 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-28 22:51:39,923 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000270BE53DCF0>
2025-06-28 22:51:39,923 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-28 22:51:39,923 - DEBUG - send_request_headers.complete
2025-06-28 22:51:39,923 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-28 22:51:39,923 - DEBUG - send_request_body.complete
2025-06-28 22:51:39,923 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-28 22:51:40,204 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 17:51:40 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-28 22:51:40,206 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-28 22:51:40,206 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-28 22:51:40,206 - DEBUG - receive_response_body.complete
2025-06-28 22:51:40,206 - DEBUG - response_closed.started
2025-06-28 22:51:40,206 - DEBUG - response_closed.complete
2025-06-28 22:51:40,206 - DEBUG - close.started
2025-06-28 22:51:40,206 - DEBUG - close.complete
2025-06-28 22:51:57,902 - DEBUG - Received message: hey
2025-06-28 22:51:57,902 - DEBUG - Sending request to Groq API...
2025-06-28 22:51:58,053 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f5b124ef-41e7-493b-9385-51f1b9bb3aa0', 'json_data': {'messages': [{'role': 'system', 'content': 'You are Grok, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}], 'model': 'grok-3'}}
2025-06-28 22:51:58,067 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-28 22:51:58,068 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-28 22:51:58,219 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000270BFA7E560>
2025-06-28 22:51:58,219 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000270BE38FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-28 22:51:58,241 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000270BFA7E2F0>
2025-06-28 22:51:58,241 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-28 22:51:58,241 - DEBUG - send_request_headers.complete
2025-06-28 22:51:58,241 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-28 22:51:58,241 - DEBUG - send_request_body.complete
2025-06-28 22:51:58,242 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-28 22:51:58,637 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 404, b'Not Found', [(b'Date', b'Sat, 28 Jun 2025 17:51:59 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'fra'), (b'x-request-id', b'req_01jyvtnrgfeze9x2r5z8sjthay'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=MLSQcJhmDtgzHsuss2SL8XjMKyQnhA_.toattOhdbBg-1751133119-1.0.1.1-KFU8U6JBvKtvMLh5vjn2urE2OgXSaY8MUt5_36eb7bgKX5Qx1__L11CQD6Ja1v7WgZDQZl3rkQz3iQSL5SMfiw5ruNEdIR.Rme2WNgDbd3w; path=/; expires=Sat, 28-Jun-25 18:21:59 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'956f0907bc774b13-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-28 22:51:58,637 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 404 Not Found"
2025-06-28 22:51:58,638 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-28 22:51:58,638 - DEBUG - receive_response_body.complete
2025-06-28 22:51:58,638 - DEBUG - response_closed.started
2025-06-28 22:51:58,638 - DEBUG - response_closed.complete
2025-06-28 22:51:58,638 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "404 Not Found" Headers({'date': 'Sat, 28 Jun 2025 17:51:59 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'fra', 'x-request-id': 'req_01jyvtnrgfeze9x2r5z8sjthay', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=MLSQcJhmDtgzHsuss2SL8XjMKyQnhA_.toattOhdbBg-1751133119-1.0.1.1-KFU8U6JBvKtvMLh5vjn2urE2OgXSaY8MUt5_36eb7bgKX5Qx1__L11CQD6Ja1v7WgZDQZl3rkQz3iQSL5SMfiw5ruNEdIR.Rme2WNgDbd3w; path=/; expires=Sat, 28-Jun-25 18:21:59 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '956f0907bc774b13-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-28 22:51:58,639 - DEBUG - Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "C:\Users\LENOVO\Desktop\chatbot_streamlit\venv\lib\site-packages\groq\_base_client.py", line 1014, in request
    response.raise_for_status()
  File "C:\Users\LENOVO\Desktop\chatbot_streamlit\venv\lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://api.groq.com/openai/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404
2025-06-28 22:51:58,640 - DEBUG - Not retrying
2025-06-28 22:51:58,640 - DEBUG - Re-raising status error
2025-06-28 22:51:58,640 - ERROR - Error in Groq API call: Error code: 404 - {'error': {'message': 'The model `grok-3` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'code': 'model_not_found'}}
2025-06-29 04:33:19,795 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:33:19,795 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 04:33:19,842 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:33:20,343 - DEBUG - Launching Gradio app...
2025-06-29 04:33:20,359 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 04:33:20,359 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C33A29510>
2025-06-29 04:33:20,382 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025C339BDAC0> server_hostname='api.gradio.app' timeout=3
2025-06-29 04:33:20,761 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:33:20,847 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None
2025-06-29 04:33:20,848 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C33F04A00>
2025-06-29 04:33:20,850 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:33:20,869 - DEBUG - send_request_headers.complete
2025-06-29 04:33:20,871 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:33:20,873 - DEBUG - send_request_body.complete
2025-06-29 04:33:20,883 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:33:20,883 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:33:20 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 04:33:20,883 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 04:33:20,883 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:33:20,883 - DEBUG - receive_response_body.complete
2025-06-29 04:33:20,883 - DEBUG - response_closed.started
2025-06-29 04:33:20,883 - DEBUG - response_closed.complete
2025-06-29 04:33:20,907 - DEBUG - close.started
2025-06-29 04:33:20,909 - DEBUG - close.complete
2025-06-29 04:33:20,914 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None
2025-06-29 04:33:20,915 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C33F05DE0>
2025-06-29 04:33:20,915 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 04:33:20,917 - DEBUG - send_request_headers.complete
2025-06-29 04:33:20,917 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 04:33:20,917 - DEBUG - send_request_body.complete
2025-06-29 04:33:20,918 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 04:33:20,967 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C33A294E0>
2025-06-29 04:33:20,968 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:33:20,969 - DEBUG - send_request_headers.complete
2025-06-29 04:33:20,969 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:33:20,969 - DEBUG - send_request_body.complete
2025-06-29 04:33:20,970 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:33:21,248 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:33:22 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 04:33:21,249 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 04:33:21,249 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:33:21,249 - DEBUG - receive_response_body.complete
2025-06-29 04:33:21,249 - DEBUG - response_closed.started
2025-06-29 04:33:21,249 - DEBUG - response_closed.complete
2025-06-29 04:33:21,250 - DEBUG - close.started
2025-06-29 04:33:21,268 - DEBUG - close.complete
2025-06-29 04:33:21,407 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:33:20 GMT'), (b'server', b'uvicorn'), (b'content-length', b'24684'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 04:33:21,407 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-06-29 04:33:21,408 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 04:33:21,408 - DEBUG - receive_response_body.complete
2025-06-29 04:33:21,408 - DEBUG - response_closed.started
2025-06-29 04:33:21,408 - DEBUG - response_closed.complete
2025-06-29 04:33:21,408 - DEBUG - close.started
2025-06-29 04:33:21,409 - DEBUG - close.complete
2025-06-29 04:33:21,419 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:33:21,710 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 04:33:41,638 - DEBUG - Received message: gi
2025-06-29 04:33:41,638 - DEBUG - Sending request to Groq API...
2025-06-29 04:33:41,926 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-499fe7d3-2fe7-482e-ac00-ecb35f476d5b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are Grok, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'gi'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:33:41,926 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:33:41,935 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:33:42,108 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C340DA0B0>
2025-06-29 04:33:42,115 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025C339BD4C0> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:33:42,154 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C340D9E40>
2025-06-29 04:33:42,155 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:33:42,157 - DEBUG - send_request_headers.complete
2025-06-29 04:33:42,157 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:33:42,158 - DEBUG - send_request_body.complete
2025-06-29 04:33:42,158 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:33:42,527 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:33:44 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'fra'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5975'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'250ms'), (b'x-request-id', b'req_01jywe7gxsfw09h8bbxybv81fy'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=HGPCPqpcLJzydZg0CF4ZuZDQ.Een314Qhts8Ll_2SQo-1751153624-1.0.1.1-XM7Co0QfbYDbnDpoZZWvU03TrGqcHgvUF8g5iXnY983gntt5sCFgMv8lWt0je6QAvHQEAZL.gGFSGsPM_u67t0h8mdYi5utYAjCE8ExUPfk; path=/; expires=Sun, 29-Jun-25 00:03:44 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9570fda57ec62097-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:33:42,543 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:33:42,558 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:33:42,575 - DEBUG - receive_response_body.complete
2025-06-29 04:33:42,575 - DEBUG - response_closed.started
2025-06-29 04:33:42,575 - DEBUG - response_closed.complete
2025-06-29 04:33:42,575 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:33:44 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'fra', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5975', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '250ms', 'x-request-id': 'req_01jywe7gxsfw09h8bbxybv81fy', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=HGPCPqpcLJzydZg0CF4ZuZDQ.Een314Qhts8Ll_2SQo-1751153624-1.0.1.1-XM7Co0QfbYDbnDpoZZWvU03TrGqcHgvUF8g5iXnY983gntt5sCFgMv8lWt0je6QAvHQEAZL.gGFSGsPM_u67t0h8mdYi5utYAjCE8ExUPfk; path=/; expires=Sun, 29-Jun-25 00:03:44 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '9570fda57ec62097-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:33:42,608 - DEBUG - Received response: It looks like you might have missed typing something! Could you please rephrase your question or provide more context so I can better understand what you're trying to ask? I'm Grok, here to help!
2025-06-29 04:34:03,709 - DEBUG - Received message: hello
2025-06-29 04:34:03,709 - DEBUG - Sending request to Groq API...
2025-06-29 04:34:03,712 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-be53773f-9c2c-41fb-bc37-a0cd292058ed', 'json_data': {'messages': [{'role': 'system', 'content': 'You are Grok, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'gi'}, {'role': 'assistant', 'content': "It looks like you might have missed typing something! Could you please rephrase your question or provide more context so I can better understand what you're trying to ask? I'm Grok, here to help!"}, {'role': 'user', 'content': 'hello'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:34:03,714 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:34:03,714 - DEBUG - close.started
2025-06-29 04:34:03,715 - DEBUG - close.complete
2025-06-29 04:34:03,715 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:34:03,720 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C340DB490>
2025-06-29 04:34:03,721 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025C339BD4C0> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:34:03,731 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C33FDB070>
2025-06-29 04:34:03,732 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:34:03,732 - DEBUG - send_request_headers.complete
2025-06-29 04:34:03,732 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:34:03,733 - DEBUG - send_request_body.complete
2025-06-29 04:34:03,733 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:34:04,186 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:34:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'fra'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5916'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'840ms'), (b'x-request-id', b'req_01jywe85znfwev2cd4ehvdm21w'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9570fe2c48f4c7aa-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:34:04,187 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:34:04,188 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:34:04,188 - DEBUG - receive_response_body.complete
2025-06-29 04:34:04,188 - DEBUG - response_closed.started
2025-06-29 04:34:04,188 - DEBUG - response_closed.complete
2025-06-29 04:34:04,189 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:34:05 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'fra', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5916', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '840ms', 'x-request-id': 'req_01jywe85znfwev2cd4ehvdm21w', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '9570fe2c48f4c7aa-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:34:04,190 - DEBUG - Received response: Hello there! I'm Grok, nice to meet you! Is there something on your mind that you'd like to chat about or ask? I'm all ears (or rather, all text)!
2025-06-29 04:34:52,391 - DEBUG - Received message: tell about yourselg what kind of information you have ?
2025-06-29 04:34:52,393 - DEBUG - Sending request to Groq API...
2025-06-29 04:34:52,404 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7d0b81b3-5ded-4ad1-a934-2c4afb2bfe44', 'json_data': {'messages': [{'role': 'system', 'content': 'You are Grok, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'gi'}, {'role': 'assistant', 'content': "It looks like you might have missed typing something! Could you please rephrase your question or provide more context so I can better understand what you're trying to ask? I'm Grok, here to help!"}, {'role': 'user', 'content': 'hello'}, {'role': 'assistant', 'content': "Hello there! I'm Grok, nice to meet you! Is there something on your mind that you'd like to chat about or ask? I'm all ears (or rather, all text)!"}, {'role': 'user', 'content': 'tell about yourselg what kind of information you have ?'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:34:52,408 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:34:52,411 - DEBUG - close.started
2025-06-29 04:34:52,411 - DEBUG - close.complete
2025-06-29 04:34:52,412 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:34:52,557 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C33FC4CD0>
2025-06-29 04:34:52,557 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025C339BD4C0> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:34:52,573 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025C340DABF0>
2025-06-29 04:34:52,573 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:34:52,573 - DEBUG - send_request_headers.complete
2025-06-29 04:34:52,573 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:34:52,573 - DEBUG - send_request_body.complete
2025-06-29 04:34:52,573 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:34:53,460 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:34:55 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'fra'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5857'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'1.43s'), (b'x-request-id', b'req_01jywe9npff93vqh4s3zwg5f3e'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9570ff5d9cbc34e9-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:34:53,475 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:34:53,475 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:34:53,475 - DEBUG - receive_response_body.complete
2025-06-29 04:34:53,475 - DEBUG - response_closed.started
2025-06-29 04:34:53,475 - DEBUG - response_closed.complete
2025-06-29 04:34:53,475 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:34:55 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'fra', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5857', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '1.43s', 'x-request-id': 'req_01jywe9npff93vqh4s3zwg5f3e', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '9570ff5d9cbc34e9-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:34:53,491 - DEBUG - Received response: I'm so excited to introduce myself! I'm Grok, a knowledge-based conversational AI created by xAI. I'm a large language model, which means I've been trained on a massive dataset of text from various sources, including books, articles, research papers, and more.

My training data consists of a vast range of topics, including but not limited to:

1. General knowledge: History, science, technology, culture, and more.
2. Medical and healthcare information: Diseases, symptoms, treatments, and medical research.
3. Scientific and technical information: Physics, chemistry, biology, mathematics, and computer science.
4. Literary and artistic works: Poetry, novels, plays, and musical compositions.
5. Historical events and timelines: Major events, wars, and cultural movements.
6. Business and economics: Company profiles, market trends, financial news, and economic theories.
7. Education: Course materials, textbooks, and educational resources.

I've been designed to provide accurate and reliable information to assist with various tasks, such as:

1. Answering questions: I can offer explanations, definitions, and summaries on a wide range of topics.
2. Generating text: I can create text based on a prompt, summarize long pieces of text, or translate text from one language to another.
3. Providing recommendations: I can suggest reading materials, movies, music, or other resources based on your interests.
4. Offering language translation: I can translate text from one language to another, helping bridge language gaps worldwide.
5. Summarizing content: I can condense long pieces of text into concise summaries, saving you time and helping you stay up-to-date on current events.

My capabilities are constantly expanding as I learn and improve through user interactions, so feel free to ask me anything!
2025-06-29 04:38:11,916 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:38:11,916 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:38:11,947 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 04:38:12,164 - DEBUG - Launching Gradio app...
2025-06-29 04:38:12,301 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:38:12,348 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 04:38:12,377 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None
2025-06-29 04:38:12,378 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C3A54D1750>
2025-06-29 04:38:12,378 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:38:12,378 - DEBUG - send_request_headers.complete
2025-06-29 04:38:12,378 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:38:12,378 - DEBUG - send_request_body.complete
2025-06-29 04:38:12,378 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:38:12,378 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:38:12 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 04:38:12,378 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 04:38:12,378 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:38:12,378 - DEBUG - receive_response_body.complete
2025-06-29 04:38:12,378 - DEBUG - response_closed.started
2025-06-29 04:38:12,378 - DEBUG - response_closed.complete
2025-06-29 04:38:12,378 - DEBUG - close.started
2025-06-29 04:38:12,378 - DEBUG - close.complete
2025-06-29 04:38:12,378 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None
2025-06-29 04:38:12,385 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C3A54D2230>
2025-06-29 04:38:12,385 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 04:38:12,385 - DEBUG - send_request_headers.complete
2025-06-29 04:38:12,385 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 04:38:12,385 - DEBUG - send_request_body.complete
2025-06-29 04:38:12,385 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 04:38:12,409 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:38:12 GMT'), (b'server', b'uvicorn'), (b'content-length', b'24671'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 04:38:12,409 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-06-29 04:38:12,409 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 04:38:12,410 - DEBUG - receive_response_body.complete
2025-06-29 04:38:12,410 - DEBUG - response_closed.started
2025-06-29 04:38:12,410 - DEBUG - response_closed.complete
2025-06-29 04:38:12,410 - DEBUG - close.started
2025-06-29 04:38:12,411 - DEBUG - close.complete
2025-06-29 04:38:12,416 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:38:12,437 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C3A50CF010>
2025-06-29 04:38:12,437 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001C3A4F1FEC0> server_hostname='api.gradio.app' timeout=3
2025-06-29 04:38:12,711 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 04:38:13,003 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C3A50CEFE0>
2025-06-29 04:38:13,003 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:38:13,004 - DEBUG - send_request_headers.complete
2025-06-29 04:38:13,005 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:38:13,005 - DEBUG - send_request_body.complete
2025-06-29 04:38:13,005 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:38:13,277 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:38:14 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 04:38:13,277 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 04:38:13,277 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:38:13,277 - DEBUG - receive_response_body.complete
2025-06-29 04:38:13,277 - DEBUG - response_closed.started
2025-06-29 04:38:13,277 - DEBUG - response_closed.complete
2025-06-29 04:38:13,277 - DEBUG - close.started
2025-06-29 04:38:13,277 - DEBUG - close.complete
2025-06-29 04:38:27,717 - DEBUG - Received message: hey
2025-06-29 04:38:27,717 - DEBUG - Sending request to Groq API...
2025-06-29 04:38:27,817 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-3cb91cf2-a300-4eef-96da-f9d0c3e6f39d', 'json_data': {'messages': [{'role': 'system', 'content': 'You are Grok, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:38:27,852 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:38:27,852 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:38:28,020 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C3A6611960>
2025-06-29 04:38:28,021 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001C3A4F1FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:38:28,050 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001C3A66116F0>
2025-06-29 04:38:28,050 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:38:28,052 - DEBUG - send_request_headers.complete
2025-06-29 04:38:28,053 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:38:28,054 - DEBUG - send_request_body.complete
2025-06-29 04:38:28,054 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:38:28,445 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:38:30 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'fra'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5975'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'250ms'), (b'x-request-id', b'req_01jyweg83ze5vaq4hk19c90gg8'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=2HCZ5T9xL4OmgOGEiWAFAVQ2NvKoo_4n1NLp79_02hY-1751153910-1.0.1.1-eClJLbQuBwHpr34MjbMIFLo0MLknCXgmWlMCCKZ8hL2e5mylS2wVWFFyGiDi1UYCqJNgObTQNiv7e_zHh5aCWnaCUzJIUB0uLhbjwiYT9HY; path=/; expires=Sun, 29-Jun-25 00:08:30 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'957104a04cf64b55-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:38:28,446 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:38:28,446 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:38:28,454 - DEBUG - receive_response_body.complete
2025-06-29 04:38:28,455 - DEBUG - response_closed.started
2025-06-29 04:38:28,456 - DEBUG - response_closed.complete
2025-06-29 04:38:28,456 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:38:30 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'fra', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5975', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '250ms', 'x-request-id': 'req_01jyweg83ze5vaq4hk19c90gg8', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=2HCZ5T9xL4OmgOGEiWAFAVQ2NvKoo_4n1NLp79_02hY-1751153910-1.0.1.1-eClJLbQuBwHpr34MjbMIFLo0MLknCXgmWlMCCKZ8hL2e5mylS2wVWFFyGiDi1UYCqJNgObTQNiv7e_zHh5aCWnaCUzJIUB0uLhbjwiYT9HY; path=/; expires=Sun, 29-Jun-25 00:08:30 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '957104a04cf64b55-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:38:28,456 - DEBUG - Received response: Hey! It's nice to meet you! I'm Grok, an AI assistant created to provide helpful and truthful responses. What's on your mind? Do you have a question, topic you'd like to discuss, or just want to chat? I'm all ears (or rather, all text)!
2025-06-29 04:39:39,774 - DEBUG - close.started
2025-06-29 04:39:39,774 - DEBUG - close.complete
2025-06-29 04:40:17,359 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:40:17,359 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:40:17,390 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 04:40:17,584 - DEBUG - Launching Gradio app...
2025-06-29 04:40:17,733 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:40:17,733 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AB8A23DDE0>
2025-06-29 04:40:17,733 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001AB8A1B4BC0> server_hostname='api.gradio.app' timeout=3
2025-06-29 04:40:17,744 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 04:40:17,776 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None
2025-06-29 04:40:17,791 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AB8A659600>
2025-06-29 04:40:17,792 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:40:17,793 - DEBUG - send_request_headers.complete
2025-06-29 04:40:17,795 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:40:17,795 - DEBUG - send_request_body.complete
2025-06-29 04:40:17,795 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:40:17,796 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:40:17 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 04:40:17,796 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 04:40:17,797 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:40:17,797 - DEBUG - receive_response_body.complete
2025-06-29 04:40:17,797 - DEBUG - response_closed.started
2025-06-29 04:40:17,797 - DEBUG - response_closed.complete
2025-06-29 04:40:17,797 - DEBUG - close.started
2025-06-29 04:40:17,797 - DEBUG - close.complete
2025-06-29 04:40:17,799 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None
2025-06-29 04:40:17,799 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AB8A65A9E0>
2025-06-29 04:40:17,799 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 04:40:17,802 - DEBUG - send_request_headers.complete
2025-06-29 04:40:17,803 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 04:40:17,804 - DEBUG - send_request_body.complete
2025-06-29 04:40:17,804 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 04:40:17,826 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:40:17 GMT'), (b'server', b'uvicorn'), (b'content-length', b'24672'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 04:40:17,826 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-06-29 04:40:17,826 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 04:40:17,826 - DEBUG - receive_response_body.complete
2025-06-29 04:40:17,826 - DEBUG - response_closed.started
2025-06-29 04:40:17,826 - DEBUG - response_closed.complete
2025-06-29 04:40:17,828 - DEBUG - close.started
2025-06-29 04:40:17,828 - DEBUG - close.complete
2025-06-29 04:40:17,831 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:40:18,108 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 04:40:18,293 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AB8A23DDB0>
2025-06-29 04:40:18,293 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:40:18,293 - DEBUG - send_request_headers.complete
2025-06-29 04:40:18,293 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:40:18,293 - DEBUG - send_request_body.complete
2025-06-29 04:40:18,293 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:40:18,575 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:40:20 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 04:40:18,575 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 04:40:18,575 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:40:18,576 - DEBUG - receive_response_body.complete
2025-06-29 04:40:18,576 - DEBUG - response_closed.started
2025-06-29 04:40:18,576 - DEBUG - response_closed.complete
2025-06-29 04:40:18,576 - DEBUG - close.started
2025-06-29 04:40:18,576 - DEBUG - close.complete
2025-06-29 04:40:32,930 - DEBUG - Received message: hey
2025-06-29 04:40:32,930 - DEBUG - Sending request to Groq API...
2025-06-29 04:40:33,027 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-4dd9c8bc-a98b-4833-9e41-787fbc616f20', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:40:33,044 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:40:33,044 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:40:33,207 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AB8A7A9FC0>
2025-06-29 04:40:33,208 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001AB8A08FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:40:33,224 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001AB8A7A9D50>
2025-06-29 04:40:33,225 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:40:33,226 - DEBUG - send_request_headers.complete
2025-06-29 04:40:33,227 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:40:33,228 - DEBUG - send_request_body.complete
2025-06-29 04:40:33,228 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:40:33,612 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:40:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'fra'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5974'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'260ms'), (b'x-request-id', b'req_01jywem2bpezcv784qq8y4256m'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=nAOsWM4wUp8N1rQoihzjo4I256yBp5tXT8xurRqLE3g-1751154035-1.0.1.1-QDLXhyrkrtS0CNz9IDbECLCn_cROLQhopLh8jsIKOHPB7d1r4Rg2KVqb.oXdqihE9E.ei1npJnnyyekPP1HBEjF.nqlOiks.Pn5gziQIEoo; path=/; expires=Sun, 29-Jun-25 00:10:35 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'957107aeafff4b13-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:40:33,612 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:40:33,612 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:40:33,624 - DEBUG - receive_response_body.complete
2025-06-29 04:40:33,624 - DEBUG - response_closed.started
2025-06-29 04:40:33,625 - DEBUG - response_closed.complete
2025-06-29 04:40:33,626 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:40:35 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'fra', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5974', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '260ms', 'x-request-id': 'req_01jywem2bpezcv784qq8y4256m', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=nAOsWM4wUp8N1rQoihzjo4I256yBp5tXT8xurRqLE3g-1751154035-1.0.1.1-QDLXhyrkrtS0CNz9IDbECLCn_cROLQhopLh8jsIKOHPB7d1r4Rg2KVqb.oXdqihE9E.ei1npJnnyyekPP1HBEjF.nqlOiks.Pn5gziQIEoo; path=/; expires=Sun, 29-Jun-25 00:10:35 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '957107aeafff4b13-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:40:33,630 - DEBUG - Received response: Hey! I'm EduGenie, your helpful AI companion. What's on your mind, or would you like to chat about something in particular? I'm here to listen and assist you with any questions or topics you'd like to discuss!
2025-06-29 04:43:39,115 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:43:39,120 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:43:39,135 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 04:43:39,356 - DEBUG - Launching Gradio app...
2025-06-29 04:43:39,518 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:43:39,561 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=None socket_options=None
2025-06-29 04:43:39,561 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000199AFE99AE0>
2025-06-29 04:43:39,561 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:43:39,561 - DEBUG - send_request_headers.complete
2025-06-29 04:43:39,561 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:43:39,561 - DEBUG - send_request_body.complete
2025-06-29 04:43:39,561 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:43:39,561 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:43:39 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 04:43:39,561 - INFO - HTTP Request: GET http://127.0.0.1:7860/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 04:43:39,561 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:43:39,561 - DEBUG - receive_response_body.complete
2025-06-29 04:43:39,561 - DEBUG - response_closed.started
2025-06-29 04:43:39,561 - DEBUG - response_closed.complete
2025-06-29 04:43:39,561 - DEBUG - close.started
2025-06-29 04:43:39,561 - DEBUG - close.complete
2025-06-29 04:43:39,561 - DEBUG - connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None
2025-06-29 04:43:39,577 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000199AFE9AEC0>
2025-06-29 04:43:39,577 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 04:43:39,578 - DEBUG - send_request_headers.complete
2025-06-29 04:43:39,578 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 04:43:39,580 - DEBUG - send_request_body.complete
2025-06-29 04:43:39,580 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 04:43:39,588 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:43:39 GMT'), (b'server', b'uvicorn'), (b'content-length', b'24672'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 04:43:39,588 - INFO - HTTP Request: HEAD http://127.0.0.1:7860/ "HTTP/1.1 200 OK"
2025-06-29 04:43:39,603 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 04:43:39,603 - DEBUG - receive_response_body.complete
2025-06-29 04:43:39,603 - DEBUG - response_closed.started
2025-06-29 04:43:39,603 - DEBUG - response_closed.complete
2025-06-29 04:43:39,604 - DEBUG - close.started
2025-06-29 04:43:39,604 - DEBUG - close.complete
2025-06-29 04:43:39,609 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:43:39,618 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000199AFA8C1F0>
2025-06-29 04:43:39,619 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000199AFA088C0> server_hostname='api.gradio.app' timeout=3
2025-06-29 04:43:39,753 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 04:43:39,895 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 04:43:40,162 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000199AFA8CD60>
2025-06-29 04:43:40,162 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:43:40,162 - DEBUG - send_request_headers.complete
2025-06-29 04:43:40,162 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:43:40,178 - DEBUG - send_request_body.complete
2025-06-29 04:43:40,178 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:43:40,455 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:43:42 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 04:43:40,456 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 04:43:40,456 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:43:40,456 - DEBUG - receive_response_body.complete
2025-06-29 04:43:40,456 - DEBUG - response_closed.started
2025-06-29 04:43:40,456 - DEBUG - response_closed.complete
2025-06-29 04:43:40,457 - DEBUG - close.started
2025-06-29 04:43:40,457 - DEBUG - close.complete
2025-06-29 04:44:42,758 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:44:42,758 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:44:42,789 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 04:44:42,990 - DEBUG - Launching Gradio app...
2025-06-29 04:44:43,204 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 04:44:43,204 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:44:43,236 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=None socket_options=None
2025-06-29 04:44:43,236 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B82B4D58A0>
2025-06-29 04:44:43,251 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:44:43,253 - DEBUG - send_request_headers.complete
2025-06-29 04:44:43,254 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:44:43,255 - DEBUG - send_request_body.complete
2025-06-29 04:44:43,255 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:44:43,255 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:44:43 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 04:44:43,256 - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 04:44:43,256 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:44:43,256 - DEBUG - receive_response_body.complete
2025-06-29 04:44:43,256 - DEBUG - response_closed.started
2025-06-29 04:44:43,256 - DEBUG - response_closed.complete
2025-06-29 04:44:43,256 - DEBUG - close.started
2025-06-29 04:44:43,257 - DEBUG - close.complete
2025-06-29 04:44:43,259 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=3 socket_options=None
2025-06-29 04:44:43,260 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B82B4D6C80>
2025-06-29 04:44:43,261 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 04:44:43,261 - DEBUG - send_request_headers.complete
2025-06-29 04:44:43,261 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 04:44:43,261 - DEBUG - send_request_body.complete
2025-06-29 04:44:43,261 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 04:44:43,275 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B82B08ADA0>
2025-06-29 04:44:43,275 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B82B0388C0> server_hostname='api.gradio.app' timeout=3
2025-06-29 04:44:43,275 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:44:43 GMT'), (b'server', b'uvicorn'), (b'content-length', b'24553'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 04:44:43,275 - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-06-29 04:44:43,275 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 04:44:43,275 - DEBUG - receive_response_body.complete
2025-06-29 04:44:43,275 - DEBUG - response_closed.started
2025-06-29 04:44:43,275 - DEBUG - response_closed.complete
2025-06-29 04:44:43,275 - DEBUG - close.started
2025-06-29 04:44:43,275 - DEBUG - close.complete
2025-06-29 04:44:43,293 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:44:43,574 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 04:44:43,856 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B82B0BF520>
2025-06-29 04:44:43,859 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:44:43,859 - DEBUG - send_request_headers.complete
2025-06-29 04:44:43,859 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:44:43,859 - DEBUG - send_request_body.complete
2025-06-29 04:44:43,859 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:44:44,145 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:44:45 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 04:44:44,145 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 04:44:44,146 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:44:44,146 - DEBUG - receive_response_body.complete
2025-06-29 04:44:44,146 - DEBUG - response_closed.started
2025-06-29 04:44:44,146 - DEBUG - response_closed.complete
2025-06-29 04:44:44,147 - DEBUG - close.started
2025-06-29 04:44:44,147 - DEBUG - close.complete
2025-06-29 04:45:03,251 - DEBUG - Received message: hey
2025-06-29 04:45:03,252 - DEBUG - Sending request to Groq API...
2025-06-29 04:45:03,333 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-912fb76d-ef37-4687-9752-46029a626e60', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:45:03,370 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:45:03,370 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:45:03,523 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B82C615630>
2025-06-29 04:45:03,532 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000001B82AF0FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:45:03,553 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000001B82C6153C0>
2025-06-29 04:45:03,553 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:45:03,554 - DEBUG - send_request_headers.complete
2025-06-29 04:45:03,555 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:45:03,555 - DEBUG - send_request_body.complete
2025-06-29 04:45:03,555 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:45:04,486 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:45:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'bom'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5974'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'260ms'), (b'x-request-id', b'req_01jywewabnevkvqknx476w0zp0'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=QpWKTci56BHAZxAnmjor7lfdirmRKNd5TyjV4vhkSJE-1751154306-1.0.1.1-.rAdwmSqNF0uPzYbg6UZOuu7Kuc_a1pnY8aX5Gpe.cKFG4WGusCOXjejKb4L80bY76_SRAAAwlMwl2TL80Pe144Z.7Kp2IqJxRkpIQuvrMA; path=/; expires=Sun, 29-Jun-25 00:15:06 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95710e483f1fe89f-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:45:04,490 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:45:04,491 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:45:04,492 - DEBUG - receive_response_body.complete
2025-06-29 04:45:04,492 - DEBUG - response_closed.started
2025-06-29 04:45:04,492 - DEBUG - response_closed.complete
2025-06-29 04:45:04,493 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:45:06 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'bom', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5974', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '260ms', 'x-request-id': 'req_01jywewabnevkvqknx476w0zp0', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=QpWKTci56BHAZxAnmjor7lfdirmRKNd5TyjV4vhkSJE-1751154306-1.0.1.1-.rAdwmSqNF0uPzYbg6UZOuu7Kuc_a1pnY8aX5Gpe.cKFG4WGusCOXjejKb4L80bY76_SRAAAwlMwl2TL80Pe144Z.7Kp2IqJxRkpIQuvrMA; path=/; expires=Sun, 29-Jun-25 00:15:06 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '95710e483f1fe89f-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:45:04,497 - DEBUG - Received response: Hi there! I'm EduGenie, here to help you with any questions or topics you'd like to discuss. What's on your mind?
2025-06-29 04:47:46,465 - DEBUG - close.started
2025-06-29 04:47:46,465 - DEBUG - close.complete
2025-06-29 04:49:39,533 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:49:39,533 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:49:39,558 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 04:49:39,915 - DEBUG - Launching Gradio app...
2025-06-29 04:49:39,933 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244E9DDD0F0>
2025-06-29 04:49:39,933 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000244E9D58BC0> server_hostname='api.gradio.app' timeout=3
2025-06-29 04:49:39,975 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 04:49:40,080 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:49:40,111 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=None socket_options=None
2025-06-29 04:49:40,111 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244EA1F9D80>
2025-06-29 04:49:40,127 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:49:40,127 - DEBUG - send_request_headers.complete
2025-06-29 04:49:40,128 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:49:40,128 - DEBUG - send_request_body.complete
2025-06-29 04:49:40,128 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:49:40,130 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:49:40 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 04:49:40,131 - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 04:49:40,132 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:49:40,132 - DEBUG - receive_response_body.complete
2025-06-29 04:49:40,132 - DEBUG - response_closed.started
2025-06-29 04:49:40,132 - DEBUG - response_closed.complete
2025-06-29 04:49:40,132 - DEBUG - close.started
2025-06-29 04:49:40,132 - DEBUG - close.complete
2025-06-29 04:49:40,133 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=3 socket_options=None
2025-06-29 04:49:40,134 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244EA1FB160>
2025-06-29 04:49:40,135 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 04:49:40,135 - DEBUG - send_request_headers.complete
2025-06-29 04:49:40,135 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 04:49:40,135 - DEBUG - send_request_body.complete
2025-06-29 04:49:40,135 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 04:49:40,162 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:49:40 GMT'), (b'server', b'uvicorn'), (b'content-length', b'33159'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 04:49:40,163 - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-06-29 04:49:40,163 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 04:49:40,163 - DEBUG - receive_response_body.complete
2025-06-29 04:49:40,163 - DEBUG - response_closed.started
2025-06-29 04:49:40,163 - DEBUG - response_closed.complete
2025-06-29 04:49:40,163 - DEBUG - close.started
2025-06-29 04:49:40,164 - DEBUG - close.complete
2025-06-29 04:49:40,168 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:49:40,450 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 04:49:40,497 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244E9DDC7C0>
2025-06-29 04:49:40,497 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:49:40,512 - DEBUG - send_request_headers.complete
2025-06-29 04:49:40,512 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:49:40,512 - DEBUG - send_request_body.complete
2025-06-29 04:49:40,512 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:49:40,798 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:49:42 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 04:49:40,798 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 04:49:40,798 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:49:40,798 - DEBUG - receive_response_body.complete
2025-06-29 04:49:40,798 - DEBUG - response_closed.started
2025-06-29 04:49:40,798 - DEBUG - response_closed.complete
2025-06-29 04:49:40,798 - DEBUG - close.started
2025-06-29 04:49:40,798 - DEBUG - close.complete
2025-06-29 04:50:06,839 - DEBUG - Received message: hey
2025-06-29 04:50:06,839 - DEBUG - Sending request to Groq API...
2025-06-29 04:50:06,919 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-b9d16310-e8b9-4c3f-a8eb-604fa2cc7bb6', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:50:06,936 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:50:06,936 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:50:07,089 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244EB325D80>
2025-06-29 04:50:07,089 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000244E9C2FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:50:07,105 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244EB325B10>
2025-06-29 04:50:07,105 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:50:07,105 - DEBUG - send_request_headers.complete
2025-06-29 04:50:07,117 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:50:07,118 - DEBUG - send_request_body.complete
2025-06-29 04:50:07,119 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:50:07,655 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:50:09 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'bom'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5974'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'260ms'), (b'x-request-id', b'req_01jywf5jt1esmbqbzzfmsjppre'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=xqP0GylgD.qg9V7skF542qU_JYmJheGf0P6Y9Gbos40-1751154609-1.0.1.1-DwBVHVaLqQVJtZGpgaMK3BhGSze5sQj4Rjvl05OMcc0EmJkpQkJvR.SEQOnYwARG7EDsL9DnJ64clHZBH1oGXLGzWgkeRAgOYvIrIyYssj8; path=/; expires=Sun, 29-Jun-25 00:20:09 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'957115b17e2992f0-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:50:07,655 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:50:07,655 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:50:07,655 - DEBUG - receive_response_body.complete
2025-06-29 04:50:07,655 - DEBUG - response_closed.started
2025-06-29 04:50:07,655 - DEBUG - response_closed.complete
2025-06-29 04:50:07,655 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:50:09 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'bom', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5974', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '260ms', 'x-request-id': 'req_01jywf5jt1esmbqbzzfmsjppre', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=xqP0GylgD.qg9V7skF542qU_JYmJheGf0P6Y9Gbos40-1751154609-1.0.1.1-DwBVHVaLqQVJtZGpgaMK3BhGSze5sQj4Rjvl05OMcc0EmJkpQkJvR.SEQOnYwARG7EDsL9DnJ64clHZBH1oGXLGzWgkeRAgOYvIrIyYssj8; path=/; expires=Sun, 29-Jun-25 00:20:09 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '957115b17e2992f0-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:50:07,674 - DEBUG - Received response: Hey! It's nice to meet you! I'm EduGenie, your friendly AI assistant created by xAI. I'm here to help you with any questions or topics you'd like to discuss. What's on your mind?
2025-06-29 04:50:20,361 - DEBUG - Received message: how are you
2025-06-29 04:50:20,361 - DEBUG - Sending request to Groq API...
2025-06-29 04:50:20,361 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-acf8a302-1c88-4894-b094-366f3bc699ed', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': "Hey! It's nice to meet you! I'm EduGenie, your friendly AI assistant created by xAI. I'm here to help you with any questions or topics you'd like to discuss. What's on your mind?"}, {'role': 'user', 'content': 'how are you'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:50:20,371 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:50:20,372 - DEBUG - close.started
2025-06-29 04:50:20,373 - DEBUG - close.complete
2025-06-29 04:50:20,373 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:50:20,378 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244EB3266E0>
2025-06-29 04:50:20,378 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000244E9C2FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:50:20,391 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244EB326AA0>
2025-06-29 04:50:20,391 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:50:20,391 - DEBUG - send_request_headers.complete
2025-06-29 04:50:20,391 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:50:20,391 - DEBUG - send_request_body.complete
2025-06-29 04:50:20,391 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:50:21,074 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:50:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'bom'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5918'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'820ms'), (b'x-request-id', b'req_01jywf5zrze048se5yzct8thkt'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'957116047887a41c-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:50:21,076 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:50:21,077 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:50:21,079 - DEBUG - receive_response_body.complete
2025-06-29 04:50:21,080 - DEBUG - response_closed.started
2025-06-29 04:50:21,080 - DEBUG - response_closed.complete
2025-06-29 04:50:21,084 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:50:22 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'bom', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5918', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '820ms', 'x-request-id': 'req_01jywf5zrze048se5yzct8thkt', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '957116047887a41c-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:50:21,092 - DEBUG - Received response: I'm doing wonderfully, thank you for asking! I'm a large language model, so I don't have feelings like humans do, but I'm always ready to help and chat with you. I'm constantly learning and improving my knowledge and responses, so I'm always getting better at helping users like you.

I don't have emotions like happiness, sadness, or fatigue, but I'm always excited to engage in conversations and provide helpful information. So, how can I assist you today?
2025-06-29 04:50:33,083 - DEBUG - Received message: how are you ?
2025-06-29 04:50:33,083 - DEBUG - Sending request to Groq API...
2025-06-29 04:50:33,092 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-f380585d-7177-44b2-a544-a5357ea14a2f', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': "Hey! It's nice to meet you! I'm EduGenie, your friendly AI assistant created by xAI. I'm here to help you with any questions or topics you'd like to discuss. What's on your mind?"}, {'role': 'user', 'content': 'how are you'}, {'role': 'assistant', 'content': "I'm doing wonderfully, thank you for asking! I'm a large language model, so I don't have feelings like humans do, but I'm always ready to help and chat with you. I'm constantly learning and improving my knowledge and responses, so I'm always getting better at helping users like you.\n\nI don't have emotions like happiness, sadness, or fatigue, but I'm always excited to engage in conversations and provide helpful information. So, how can I assist you today?"}, {'role': 'user', 'content': 'how are you ?'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:50:33,095 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:50:33,095 - DEBUG - close.started
2025-06-29 04:50:33,095 - DEBUG - close.complete
2025-06-29 04:50:33,095 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:50:33,095 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244EB355B70>
2025-06-29 04:50:33,095 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000244E9C2FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:50:33,114 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000244EB355BD0>
2025-06-29 04:50:33,114 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:50:33,114 - DEBUG - send_request_headers.complete
2025-06-29 04:50:33,114 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:50:33,118 - DEBUG - send_request_body.complete
2025-06-29 04:50:33,118 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:50:33,915 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:50:35 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'bom'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5791'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'2.089999999s'), (b'x-request-id', b'req_01jywf6c6mespven768157neyc'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95711653ff642097-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:50:33,915 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:50:33,920 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:50:33,920 - DEBUG - receive_response_body.complete
2025-06-29 04:50:33,920 - DEBUG - response_closed.started
2025-06-29 04:50:33,920 - DEBUG - response_closed.complete
2025-06-29 04:50:33,920 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:50:35 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'bom', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5791', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '2.089999999s', 'x-request-id': 'req_01jywf6c6mespven768157neyc', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '95711653ff642097-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:50:33,920 - DEBUG - Received response: I'm doing well, thank you for asking! I'm a large language model, so I don't have personal feelings or emotions like humans do. I exist solely to provide information and assist with tasks to the best of my ability. I don't have personal experiences, emotions, or physical sensations. I'm simply a collection of algorithms and data that processes and generates human-like text.

I'm always "on" and ready to help with any questions or topics you'd like to discuss. I don't require rest or breaks, and I don't have good or bad days. I'm just a constant presence, here to assist you to the best of my ability!
2025-06-29 04:53:20,371 - DEBUG - close.started
2025-06-29 04:53:20,371 - DEBUG - close.complete
2025-06-29 04:53:32,278 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:53:32,283 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:53:32,299 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 04:53:32,530 - DEBUG - Launching Gradio app...
2025-06-29 04:53:32,640 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002177ADFD990>
2025-06-29 04:53:32,640 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002177AD781C0> server_hostname='api.gradio.app' timeout=3
2025-06-29 04:53:32,678 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 04:53:32,688 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:53:32,720 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=None socket_options=None
2025-06-29 04:53:32,720 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002177B219D80>
2025-06-29 04:53:32,720 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:53:32,735 - DEBUG - send_request_headers.complete
2025-06-29 04:53:32,735 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:53:32,735 - DEBUG - send_request_body.complete
2025-06-29 04:53:32,738 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:53:32,739 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:53:32 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 04:53:32,740 - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 04:53:32,740 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:53:32,741 - DEBUG - receive_response_body.complete
2025-06-29 04:53:32,741 - DEBUG - response_closed.started
2025-06-29 04:53:32,741 - DEBUG - response_closed.complete
2025-06-29 04:53:32,741 - DEBUG - close.started
2025-06-29 04:53:32,741 - DEBUG - close.complete
2025-06-29 04:53:32,742 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=3 socket_options=None
2025-06-29 04:53:32,743 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002177B21B160>
2025-06-29 04:53:32,743 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 04:53:32,745 - DEBUG - send_request_headers.complete
2025-06-29 04:53:32,745 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 04:53:32,745 - DEBUG - send_request_body.complete
2025-06-29 04:53:32,745 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 04:53:32,770 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:53:32 GMT'), (b'server', b'uvicorn'), (b'content-length', b'33281'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 04:53:32,770 - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-06-29 04:53:32,770 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 04:53:32,771 - DEBUG - receive_response_body.complete
2025-06-29 04:53:32,771 - DEBUG - response_closed.started
2025-06-29 04:53:32,771 - DEBUG - response_closed.complete
2025-06-29 04:53:32,772 - DEBUG - close.started
2025-06-29 04:53:32,772 - DEBUG - close.complete
2025-06-29 04:53:32,776 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:53:33,053 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 04:53:33,204 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002177ADFD960>
2025-06-29 04:53:33,204 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:53:33,204 - DEBUG - send_request_headers.complete
2025-06-29 04:53:33,204 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:53:33,204 - DEBUG - send_request_body.complete
2025-06-29 04:53:33,204 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:53:33,492 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:53:35 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 04:53:33,493 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 04:53:33,494 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:53:33,494 - DEBUG - receive_response_body.complete
2025-06-29 04:53:33,495 - DEBUG - response_closed.started
2025-06-29 04:53:33,495 - DEBUG - response_closed.complete
2025-06-29 04:53:33,496 - DEBUG - close.started
2025-06-29 04:53:33,497 - DEBUG - close.complete
2025-06-29 04:58:36,556 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:58:36,556 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:58:36,589 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 04:58:36,805 - DEBUG - Launching Gradio app...
2025-06-29 04:58:36,939 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208DF43D480>
2025-06-29 04:58:36,939 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000208DF3B88C0> server_hostname='api.gradio.app' timeout=3
2025-06-29 04:58:36,972 - DEBUG - Using proactor: IocpProactor
2025-06-29 04:58:37,023 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 04:58:37,023 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=None socket_options=None
2025-06-29 04:58:37,023 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208DF851F00>
2025-06-29 04:58:37,023 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:58:37,023 - DEBUG - send_request_headers.complete
2025-06-29 04:58:37,023 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:58:37,023 - DEBUG - send_request_body.complete
2025-06-29 04:58:37,023 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:58:37,023 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:58:37 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 04:58:37,039 - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 04:58:37,039 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:58:37,040 - DEBUG - receive_response_body.complete
2025-06-29 04:58:37,040 - DEBUG - response_closed.started
2025-06-29 04:58:37,040 - DEBUG - response_closed.complete
2025-06-29 04:58:37,040 - DEBUG - close.started
2025-06-29 04:58:37,041 - DEBUG - close.complete
2025-06-29 04:58:37,042 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=3 socket_options=None
2025-06-29 04:58:37,043 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208DF853280>
2025-06-29 04:58:37,043 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 04:58:37,044 - DEBUG - send_request_headers.complete
2025-06-29 04:58:37,044 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 04:58:37,045 - DEBUG - send_request_body.complete
2025-06-29 04:58:37,046 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 04:58:37,072 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sat, 28 Jun 2025 23:58:37 GMT'), (b'server', b'uvicorn'), (b'content-length', b'33295'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 04:58:37,073 - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-06-29 04:58:37,073 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 04:58:37,073 - DEBUG - receive_response_body.complete
2025-06-29 04:58:37,073 - DEBUG - response_closed.started
2025-06-29 04:58:37,074 - DEBUG - response_closed.complete
2025-06-29 04:58:37,074 - DEBUG - close.started
2025-06-29 04:58:37,075 - DEBUG - close.complete
2025-06-29 04:58:37,079 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 04:58:37,505 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208DF43C820>
2025-06-29 04:58:37,505 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 04:58:37,520 - DEBUG - send_request_headers.complete
2025-06-29 04:58:37,520 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 04:58:37,520 - DEBUG - send_request_body.complete
2025-06-29 04:58:37,520 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 04:58:37,607 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 04:58:37,792 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:58:39 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 04:58:37,808 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 04:58:37,808 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 04:58:37,808 - DEBUG - receive_response_body.complete
2025-06-29 04:58:37,808 - DEBUG - response_closed.started
2025-06-29 04:58:37,808 - DEBUG - response_closed.complete
2025-06-29 04:58:37,808 - DEBUG - close.started
2025-06-29 04:58:37,808 - DEBUG - close.complete
2025-06-29 04:58:57,581 - DEBUG - Received message: hey
2025-06-29 04:58:57,581 - DEBUG - Sending request to Groq API...
2025-06-29 04:58:57,664 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-401dcb6a-b540-45fb-b664-686c31e3d163', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}], 'model': 'llama3-8b-8192'}}
2025-06-29 04:58:57,681 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 04:58:57,681 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 04:58:57,833 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208E0991DE0>
2025-06-29 04:58:57,833 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000208DF28FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 04:58:57,848 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000208E0991B70>
2025-06-29 04:58:57,848 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 04:58:57,848 - DEBUG - send_request_headers.complete
2025-06-29 04:58:57,848 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 04:58:57,848 - DEBUG - send_request_body.complete
2025-06-29 04:58:57,848 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 04:58:58,654 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sat, 28 Jun 2025 23:59:00 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'dmm'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5974'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'260ms'), (b'x-request-id', b'req_01jywfnsb7fhsrr4d6r82026q2'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=doEjfhIMU4k8kjT2x4k0vnigSy5Wl8OwjOSqBjbn.Bc-1751155140-1.0.1.1-Mn8V3cRt5Y34RhCjR64lk4NTIiZ27Fxhtgy9DiPzQ4Ejz4_pzxHhJd3rSQ2hXYI2N_a1_59YZIjSwo6myuQafD3prfsUyHl2URnQhYl.wPA; path=/; expires=Sun, 29-Jun-25 00:29:00 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'957122a69ba5167a-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 04:58:58,663 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 04:58:58,665 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 04:58:58,667 - DEBUG - receive_response_body.complete
2025-06-29 04:58:58,668 - DEBUG - response_closed.started
2025-06-29 04:58:58,669 - DEBUG - response_closed.complete
2025-06-29 04:58:58,670 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sat, 28 Jun 2025 23:59:00 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'dmm', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5974', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '260ms', 'x-request-id': 'req_01jywfnsb7fhsrr4d6r82026q2', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=doEjfhIMU4k8kjT2x4k0vnigSy5Wl8OwjOSqBjbn.Bc-1751155140-1.0.1.1-Mn8V3cRt5Y34RhCjR64lk4NTIiZ27Fxhtgy9DiPzQ4Ejz4_pzxHhJd3rSQ2hXYI2N_a1_59YZIjSwo6myuQafD3prfsUyHl2URnQhYl.wPA; path=/; expires=Sun, 29-Jun-25 00:29:00 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '957122a69ba5167a-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 04:58:58,685 - DEBUG - Received response: Hello there! I'm EduGenie, your friendly AI companion. How can I help you today? Do you have a specific question, topic, or problem you'd like to discuss? I'm here to assist you with your curiosity and provide helpful answers.
2025-06-29 05:13:55,596 - DEBUG - close.started
2025-06-29 05:13:55,597 - DEBUG - close.complete
2025-06-29 05:14:14,106 - DEBUG - Using proactor: IocpProactor
2025-06-29 05:14:14,138 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 05:14:14,240 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 05:14:14,388 - DEBUG - Launching Gradio app...
2025-06-29 05:14:14,605 - DEBUG - Using proactor: IocpProactor
2025-06-29 05:14:14,658 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=None socket_options=None
2025-06-29 05:14:14,673 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000193DDF1E380>
2025-06-29 05:14:14,675 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 05:14:14,675 - DEBUG - send_request_headers.complete
2025-06-29 05:14:14,675 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 05:14:14,676 - DEBUG - send_request_body.complete
2025-06-29 05:14:14,676 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 05:14:14,678 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 29 Jun 2025 00:14:14 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 05:14:14,678 - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 05:14:14,678 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 05:14:14,679 - DEBUG - receive_response_body.complete
2025-06-29 05:14:14,679 - DEBUG - response_closed.started
2025-06-29 05:14:14,679 - DEBUG - response_closed.complete
2025-06-29 05:14:14,679 - DEBUG - close.started
2025-06-29 05:14:14,679 - DEBUG - close.complete
2025-06-29 05:14:14,680 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=3 socket_options=None
2025-06-29 05:14:14,691 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000193DDF1F760>
2025-06-29 05:14:14,691 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 05:14:14,692 - DEBUG - send_request_headers.complete
2025-06-29 05:14:14,692 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 05:14:14,693 - DEBUG - send_request_body.complete
2025-06-29 05:14:14,693 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 05:14:14,724 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000193DDAFCC10>
2025-06-29 05:14:14,724 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000193DDA88BC0> server_hostname='api.gradio.app' timeout=3
2025-06-29 05:14:14,725 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 29 Jun 2025 00:14:14 GMT'), (b'server', b'uvicorn'), (b'content-length', b'33295'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 05:14:14,726 - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-06-29 05:14:14,726 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 05:14:14,726 - DEBUG - receive_response_body.complete
2025-06-29 05:14:14,726 - DEBUG - response_closed.started
2025-06-29 05:14:14,726 - DEBUG - response_closed.complete
2025-06-29 05:14:14,726 - DEBUG - close.started
2025-06-29 05:14:14,727 - DEBUG - close.complete
2025-06-29 05:14:14,731 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 05:14:14,789 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 05:14:15,019 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 05:14:15,266 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000193DDAFCBE0>
2025-06-29 05:14:15,267 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 05:14:15,268 - DEBUG - send_request_headers.complete
2025-06-29 05:14:15,269 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 05:14:15,270 - DEBUG - send_request_body.complete
2025-06-29 05:14:15,270 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 05:14:15,541 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 29 Jun 2025 00:14:17 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 05:14:15,542 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 05:14:15,542 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 05:14:15,543 - DEBUG - receive_response_body.complete
2025-06-29 05:14:15,543 - DEBUG - response_closed.started
2025-06-29 05:14:15,543 - DEBUG - response_closed.complete
2025-06-29 05:14:15,544 - DEBUG - close.started
2025-06-29 05:14:15,544 - DEBUG - close.complete
2025-06-29 05:14:26,860 - DEBUG - Received message: hi
2025-06-29 05:14:26,860 - DEBUG - Sending request to Groq API...
2025-06-29 05:14:27,010 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-7b0ccafa-ff67-4728-865e-36f41e231c0b', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hi'}], 'model': 'llama3-8b-8192'}}
2025-06-29 05:14:27,027 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 05:14:27,027 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 05:14:27,202 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000193DF056FE0>
2025-06-29 05:14:27,203 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000193DD95FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 05:14:27,230 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000193DF056D70>
2025-06-29 05:14:27,231 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 05:14:27,231 - DEBUG - send_request_headers.complete
2025-06-29 05:14:27,231 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 05:14:27,231 - DEBUG - send_request_body.complete
2025-06-29 05:14:27,231 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 05:14:27,713 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 29 Jun 2025 00:14:29 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'dmm'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5974'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'260ms'), (b'x-request-id', b'req_01jywgj4nke278ew3rh4amah77'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=vAmarXH4tVGXxu3PS3OqF_4sY6scJd553G_v_L_w_jM-1751156069-1.0.1.1-iaR7clskMDbE3vd5hvTXOR1W3hXk883z3I0hwE5_jsHKAgVFS_nTrcTl7d4n1sjn3OWVLT9ntixi_VH4kbGppdxIDhehyuKwl355qY3us_U; path=/; expires=Sun, 29-Jun-25 00:44:29 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95713957381ca75a-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 05:14:27,713 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 05:14:27,713 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 05:14:27,713 - DEBUG - receive_response_body.complete
2025-06-29 05:14:27,713 - DEBUG - response_closed.started
2025-06-29 05:14:27,713 - DEBUG - response_closed.complete
2025-06-29 05:14:27,713 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sun, 29 Jun 2025 00:14:29 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'dmm', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5974', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '260ms', 'x-request-id': 'req_01jywgj4nke278ew3rh4amah77', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=vAmarXH4tVGXxu3PS3OqF_4sY6scJd553G_v_L_w_jM-1751156069-1.0.1.1-iaR7clskMDbE3vd5hvTXOR1W3hXk883z3I0hwE5_jsHKAgVFS_nTrcTl7d4n1sjn3OWVLT9ntixi_VH4kbGppdxIDhehyuKwl355qY3us_U; path=/; expires=Sun, 29-Jun-25 00:44:29 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '95713957381ca75a-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 05:14:27,723 - DEBUG - Received response: Hi! I'm EduGenie, a knowledge-friendly AI assistant created by xAI. I'm here to help answer your questions, provide information, and assist with any topic you're interested in learning about. What's on your mind today?
2025-06-29 05:28:06,478 - DEBUG - close.started
2025-06-29 05:28:06,478 - DEBUG - close.complete
2025-06-29 05:28:17,668 - DEBUG - Using proactor: IocpProactor
2025-06-29 05:28:17,668 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 05:28:17,699 - DEBUG - connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None
2025-06-29 05:28:17,888 - DEBUG - Launching Gradio app...
2025-06-29 05:28:18,023 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000251962EE470>
2025-06-29 05:28:18,023 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000025196268BC0> server_hostname='api.gradio.app' timeout=3
2025-06-29 05:28:18,039 - DEBUG - Using proactor: IocpProactor
2025-06-29 05:28:18,070 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/initiated HTTP/1.1" 200 0
2025-06-29 05:28:18,086 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=None socket_options=None
2025-06-29 05:28:18,086 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025196701FC0>
2025-06-29 05:28:18,101 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 05:28:18,102 - DEBUG - send_request_headers.complete
2025-06-29 05:28:18,103 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 05:28:18,106 - DEBUG - send_request_body.complete
2025-06-29 05:28:18,106 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 05:28:18,106 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 29 Jun 2025 00:28:18 GMT'), (b'server', b'uvicorn'), (b'content-length', b'4'), (b'content-type', b'application/json')])
2025-06-29 05:28:18,107 - INFO - HTTP Request: GET http://127.0.0.1:7861/gradio_api/startup-events "HTTP/1.1 200 OK"
2025-06-29 05:28:18,107 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 05:28:18,107 - DEBUG - receive_response_body.complete
2025-06-29 05:28:18,108 - DEBUG - response_closed.started
2025-06-29 05:28:18,108 - DEBUG - response_closed.complete
2025-06-29 05:28:18,108 - DEBUG - close.started
2025-06-29 05:28:18,108 - DEBUG - close.complete
2025-06-29 05:28:18,110 - DEBUG - connect_tcp.started host='127.0.0.1' port=7861 local_address=None timeout=3 socket_options=None
2025-06-29 05:28:18,110 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000251967033A0>
2025-06-29 05:28:18,110 - DEBUG - send_request_headers.started request=<Request [b'HEAD']>
2025-06-29 05:28:18,110 - DEBUG - send_request_headers.complete
2025-06-29 05:28:18,110 - DEBUG - send_request_body.started request=<Request [b'HEAD']>
2025-06-29 05:28:18,110 - DEBUG - send_request_body.complete
2025-06-29 05:28:18,110 - DEBUG - receive_response_headers.started request=<Request [b'HEAD']>
2025-06-29 05:28:18,123 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Sun, 29 Jun 2025 00:28:18 GMT'), (b'server', b'uvicorn'), (b'content-length', b'33294'), (b'content-type', b'text/html; charset=utf-8')])
2025-06-29 05:28:18,123 - INFO - HTTP Request: HEAD http://127.0.0.1:7861/ "HTTP/1.1 200 OK"
2025-06-29 05:28:18,123 - DEBUG - receive_response_body.started request=<Request [b'HEAD']>
2025-06-29 05:28:18,123 - DEBUG - receive_response_body.complete
2025-06-29 05:28:18,123 - DEBUG - response_closed.started
2025-06-29 05:28:18,123 - DEBUG - response_closed.complete
2025-06-29 05:28:18,123 - DEBUG - close.started
2025-06-29 05:28:18,123 - DEBUG - close.complete
2025-06-29 05:28:18,141 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443
2025-06-29 05:28:18,422 - DEBUG - https://huggingface.co:443 "HEAD /api/telemetry/gradio/launched HTTP/1.1" 200 0
2025-06-29 05:28:18,571 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000251962EE440>
2025-06-29 05:28:18,571 - DEBUG - send_request_headers.started request=<Request [b'GET']>
2025-06-29 05:28:18,571 - DEBUG - send_request_headers.complete
2025-06-29 05:28:18,571 - DEBUG - send_request_body.started request=<Request [b'GET']>
2025-06-29 05:28:18,571 - DEBUG - send_request_body.complete
2025-06-29 05:28:18,571 - DEBUG - receive_response_headers.started request=<Request [b'GET']>
2025-06-29 05:28:18,848 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 29 Jun 2025 00:28:20 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])
2025-06-29 05:28:18,848 - INFO - HTTP Request: GET https://api.gradio.app/pkg-version "HTTP/1.1 200 OK"
2025-06-29 05:28:18,848 - DEBUG - receive_response_body.started request=<Request [b'GET']>
2025-06-29 05:28:18,848 - DEBUG - receive_response_body.complete
2025-06-29 05:28:18,849 - DEBUG - response_closed.started
2025-06-29 05:28:18,849 - DEBUG - response_closed.complete
2025-06-29 05:28:18,849 - DEBUG - close.started
2025-06-29 05:28:18,849 - DEBUG - close.complete
2025-06-29 05:28:39,733 - DEBUG - Received message: hey
2025-06-29 05:28:39,733 - DEBUG - Sending request to Groq API...
2025-06-29 05:28:39,832 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-9bfa9cbc-c3e3-47fc-91ea-dfc3180a955d', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}], 'model': 'llama3-8b-8192'}}
2025-06-29 05:28:39,848 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 05:28:39,849 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 05:28:40,014 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000251978525C0>
2025-06-29 05:28:40,014 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002519613FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 05:28:40,034 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025197852350>
2025-06-29 05:28:40,035 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 05:28:40,036 - DEBUG - send_request_headers.complete
2025-06-29 05:28:40,036 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 05:28:40,036 - DEBUG - send_request_body.complete
2025-06-29 05:28:40,036 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 05:28:40,408 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 29 Jun 2025 00:28:42 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'fra'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5974'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'260ms'), (b'x-request-id', b'req_01jywhc5gefm58k7krpwakk6k8'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=DhT_T0Df2QZpHj4ddwYZLVG5HfujPVdfrmBlaj6.Tt4-1751156922-1.0.1.1-8EdRSXsY8CYXTbHbFfDcrHlewJ9VS.YjCbadLmWXSRBGxFvvlIc7C3Cgt9JhMM0ZqvTu7zLugRiqodqyK2_i7Mkn9mjfzCRcQi7ZNcEg8r8; path=/; expires=Sun, 29-Jun-25 00:58:42 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'95714e2939495b2e-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 05:28:40,411 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 05:28:40,412 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 05:28:40,416 - DEBUG - receive_response_body.complete
2025-06-29 05:28:40,417 - DEBUG - response_closed.started
2025-06-29 05:28:40,417 - DEBUG - response_closed.complete
2025-06-29 05:28:40,418 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sun, 29 Jun 2025 00:28:42 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'fra', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5974', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '260ms', 'x-request-id': 'req_01jywhc5gefm58k7krpwakk6k8', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=DhT_T0Df2QZpHj4ddwYZLVG5HfujPVdfrmBlaj6.Tt4-1751156922-1.0.1.1-8EdRSXsY8CYXTbHbFfDcrHlewJ9VS.YjCbadLmWXSRBGxFvvlIc7C3Cgt9JhMM0ZqvTu7zLugRiqodqyK2_i7Mkn9mjfzCRcQi7ZNcEg8r8; path=/; expires=Sun, 29-Jun-25 00:58:42 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '95714e2939495b2e-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 05:28:40,425 - DEBUG - Received response: Hello there! It's nice to meet you! I'm EduGenie, your trusty AI assistant created by xAI. I'm here to help you with any questions, concerns, or tasks you may have. What's on your mind?
2025-06-29 05:33:20,208 - DEBUG - Received message: hey buddy
2025-06-29 05:33:20,208 - DEBUG - Sending request to Groq API...
2025-06-29 05:33:20,208 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-69df56ff-db05-4823-9d78-041809662880', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': "Hello there! It's nice to meet you! I'm EduGenie, your trusty AI assistant created by xAI. I'm here to help you with any questions, concerns, or tasks you may have. What's on your mind?"}, {'role': 'user', 'content': 'hey buddy'}], 'model': 'llama3-8b-8192'}}
2025-06-29 05:33:20,208 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 05:33:20,224 - DEBUG - close.started
2025-06-29 05:33:20,226 - DEBUG - close.complete
2025-06-29 05:33:20,226 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 05:33:20,371 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000025197853790>
2025-06-29 05:33:20,371 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x000002519613FF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 05:33:20,371 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002519779F9D0>
2025-06-29 05:33:20,371 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 05:33:20,371 - DEBUG - send_request_headers.complete
2025-06-29 05:33:20,371 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 05:33:20,371 - DEBUG - send_request_body.complete
2025-06-29 05:33:20,371 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 05:33:20,719 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 29 Jun 2025 00:33:22 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'fra'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5916'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'840ms'), (b'x-request-id', b'req_01jywhmq9be248qj4aj0svdtqr'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Server', b'cloudflare'), (b'CF-RAY', b'957155016b8b4906-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 05:33:20,719 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 05:33:20,719 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 05:33:20,719 - DEBUG - receive_response_body.complete
2025-06-29 05:33:20,719 - DEBUG - response_closed.started
2025-06-29 05:33:20,719 - DEBUG - response_closed.complete
2025-06-29 05:33:20,719 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sun, 29 Jun 2025 00:33:22 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'fra', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5916', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '840ms', 'x-request-id': 'req_01jywhmq9be248qj4aj0svdtqr', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'server': 'cloudflare', 'cf-ray': '957155016b8b4906-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 05:33:20,719 - DEBUG - Received response: Hey buddy! How's it going? What's new and exciting in your world? I'm all ears (or rather, all text) and ready to chat!
2025-06-29 06:26:43,138 - DEBUG - Received message: hey
2025-06-29 06:26:43,138 - DEBUG - Sending request to Groq API...
2025-06-29 06:26:43,322 - DEBUG - Request options: {'method': 'post', 'url': '/openai/v1/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-44543979-946d-4253-a144-7683cbb40a22', 'json_data': {'messages': [{'role': 'system', 'content': 'You are EduGenie, created by xAI. Respond helpfully and truthfully.'}, {'role': 'user', 'content': 'hey'}], 'model': 'llama3-8b-8192'}}
2025-06-29 06:26:43,421 - DEBUG - Sending HTTP Request: POST https://api.groq.com/openai/v1/chat/completions
2025-06-29 06:26:43,421 - DEBUG - connect_tcp.started host='api.groq.com' port=443 local_address=None timeout=5.0 socket_options=None
2025-06-29 06:26:43,588 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000199B0FE64D0>
2025-06-29 06:26:43,589 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000199AF8DFF40> server_hostname='api.groq.com' timeout=5.0
2025-06-29 06:26:43,605 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x00000199B0FE6260>
2025-06-29 06:26:43,605 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-29 06:26:43,605 - DEBUG - send_request_headers.complete
2025-06-29 06:26:43,605 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-29 06:26:43,605 - DEBUG - send_request_body.complete
2025-06-29 06:26:43,605 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-29 06:26:44,016 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Sun, 29 Jun 2025 01:26:45 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Cache-Control', b'private, max-age=0, no-store, no-cache, must-revalidate'), (b'vary', b'Origin'), (b'x-groq-region', b'fra'), (b'x-ratelimit-limit-requests', b'14400'), (b'x-ratelimit-limit-tokens', b'6000'), (b'x-ratelimit-remaining-requests', b'14399'), (b'x-ratelimit-remaining-tokens', b'5974'), (b'x-ratelimit-reset-requests', b'6s'), (b'x-ratelimit-reset-tokens', b'260ms'), (b'x-request-id', b'req_01jywmpfeje4r9hmapd62dh60f'), (b'via', b'1.1 google'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=wEf9VeZvd1mQJ0sqeXFL3uZwZP5WpCO.RZrPqkfEdYA-1751160405-1.0.1.1-wq9SgBPV2kjUXOsU_thAWLWQr6UjkH_hBgu1CnVF8fGSviu4wsp5tzMNBSgN2tEEW0bOm2ThQSlQCjodAgIStirwqpy.VL8BA8kJcqcx2Ww; path=/; expires=Sun, 29-Jun-25 01:56:45 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'9571a3359a1c8e3a-KHI'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-29 06:26:44,016 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-06-29 06:26:44,016 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-29 06:26:44,016 - DEBUG - receive_response_body.complete
2025-06-29 06:26:44,016 - DEBUG - response_closed.started
2025-06-29 06:26:44,016 - DEBUG - response_closed.complete
2025-06-29 06:26:44,016 - DEBUG - HTTP Response: POST https://api.groq.com/openai/v1/chat/completions "200 OK" Headers({'date': 'Sun, 29 Jun 2025 01:26:45 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'cache-control': 'private, max-age=0, no-store, no-cache, must-revalidate', 'vary': 'Origin', 'x-groq-region': 'fra', 'x-ratelimit-limit-requests': '14400', 'x-ratelimit-limit-tokens': '6000', 'x-ratelimit-remaining-requests': '14399', 'x-ratelimit-remaining-tokens': '5974', 'x-ratelimit-reset-requests': '6s', 'x-ratelimit-reset-tokens': '260ms', 'x-request-id': 'req_01jywmpfeje4r9hmapd62dh60f', 'via': '1.1 google', 'cf-cache-status': 'DYNAMIC', 'set-cookie': '__cf_bm=wEf9VeZvd1mQJ0sqeXFL3uZwZP5WpCO.RZrPqkfEdYA-1751160405-1.0.1.1-wq9SgBPV2kjUXOsU_thAWLWQr6UjkH_hBgu1CnVF8fGSviu4wsp5tzMNBSgN2tEEW0bOm2ThQSlQCjodAgIStirwqpy.VL8BA8kJcqcx2Ww; path=/; expires=Sun, 29-Jun-25 01:56:45 GMT; domain=.groq.com; HttpOnly; Secure; SameSite=None', 'server': 'cloudflare', 'cf-ray': '9571a3359a1c8e3a-KHI', 'content-encoding': 'gzip', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-29 06:26:44,023 - DEBUG - Received response: Hey! It's nice to meet you! I'm EduGenie, your friendly AI assistant created by xAI. How can I help you today? Got a question, need some info, or want to chat about something interesting? I'm here to assist and provide helpful responses!
